"""
ASR Worker with Hybrid Sentence Segmentation

å½“å‰å®ç°ï¼šåŸºäº Faster-Whisper çš„è¯­éŸ³è¯†åˆ«å¼•æ“
æ”¯æŒæµå¼è¯†åˆ«å’Œæ··åˆåˆ†å¥ç­–ç•¥ã€‚

ã€ä¼˜åŒ–ç‰ˆæœ¬ã€‘
æ ¸å¿ƒæ”¹è¿›ï¼š
1. æ»‘åŠ¨çª—å£è¯†åˆ« - é¿å…é‡å¤è¯†åˆ«æ•´ä¸ªç´¯ç§¯çš„éŸ³é¢‘
2. ç´¯ç§¯è¶³å¤ŸéŸ³é¢‘å†è¯†åˆ« - æé«˜è¯†åˆ«è´¨é‡å’Œæ ‡ç‚¹å‡†ç¡®æ€§
3. å¢é‡æ–‡æœ¬æå– - åªè¾“å‡ºæ–°è¯†åˆ«çš„éƒ¨åˆ†
4. æ™ºèƒ½åˆ†å¥ - åŸºäºæ ‡ç‚¹å’Œè¯­ä¹‰è¾¹ç•Œ

IPC åè®®ï¼š
- è¾“å…¥ï¼šstreaming_chunk, batch_file, reset_session, force_commit
- è¾“å‡ºï¼š
  - partial: å®æ—¶å­—å¹•ï¼ˆå¢é‡æ–‡æœ¬ï¼‰
  - sentence_complete: å®Œæ•´å¥å­ï¼ˆè§¦å‘å­˜åº“ï¼‰
"""

import base64
import json
import os
import re
import sys
import time
import traceback
from collections import deque
from dataclasses import dataclass, field
from typing import Deque, Dict, List, Optional, Tuple

import numpy as np
from faster_whisper import WhisperModel

# ==============================================================================
# æ ¸å¿ƒä¿®å¤ï¼šOS çº§åˆ«çš„æ–‡ä»¶æè¿°ç¬¦é‡å®šå‘
# ==============================================================================
ipc_fd = os.dup(sys.stdout.fileno())
ipc_channel = os.fdopen(ipc_fd, "w", buffering=1, encoding="utf-8")
os.dup2(sys.stderr.fileno(), sys.stdout.fileno())
sys.stdout = sys.stderr


def send_ipc_message(data):
    """å‘é€ JSON æ¶ˆæ¯åˆ° Node.js"""
    try:
        json_str = json.dumps(data, ensure_ascii=False)
        ipc_channel.write(json_str + "\n")
        ipc_channel.flush()
    except Exception as exc:
        sys.stderr.write(f"[IPC Error] Failed to send: {exc}\n")
        sys.stderr.flush()


# ==============================================================================
# ç¯å¢ƒå˜é‡é…ç½®
# ==============================================================================
os.environ["CUDA_VISIBLE_DEVICES"] = "-1"
os.environ["OMP_NUM_THREADS"] = "1"
os.environ["MKL_NUM_THREADS"] = "1"
os.environ["OPENBLAS_NUM_THREADS"] = "1"
os.environ.setdefault("TQDM_DISABLE", "1")

# ==============================================================================
# ASR é…ç½®ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
# ==============================================================================
SAMPLE_RATE = int(os.environ.get("ASR_SAMPLE_RATE", "16000"))

# ã€ä¼˜åŒ–ã€‘æ»‘åŠ¨çª—å£é…ç½®
WINDOW_SECONDS = float(os.environ.get("ASR_WINDOW_SECONDS", "8"))  # è¯†åˆ«çª—å£å¤§å°ï¼ˆç§’ï¼‰
MIN_NEW_AUDIO_SECONDS = float(os.environ.get("ASR_MIN_NEW_AUDIO", "1.5"))  # æœ€å°‘ç´¯ç§¯å¤šå°‘æ–°éŸ³é¢‘æ‰è§¦å‘è¯†åˆ«
MAX_BUFFER_SECONDS = float(os.environ.get("ASR_BUFFER_SECONDS", "30"))  # æœ€å¤§ç¼“å†²ï¼ˆé˜²æ­¢å†…å­˜æº¢å‡ºï¼‰

WINDOW_SAMPLES = int(WINDOW_SECONDS * SAMPLE_RATE)
MIN_NEW_AUDIO_SAMPLES = int(MIN_NEW_AUDIO_SECONDS * SAMPLE_RATE)
MAX_BUFFER_SAMPLES = int(MAX_BUFFER_SECONDS * SAMPLE_RATE)

DEFAULT_MODEL_FALLBACK = "medium"

# åˆ†å¥é…ç½®
SENTENCE_END_PUNCTUATION = set("ã€‚ï¼ï¼Ÿ!?.ï¼›;")  # å¥æœ«æ ‡ç‚¹
CLAUSE_PUNCTUATION = set("ï¼Œ,ã€ï¼š:")  # åˆ†å¥æ ‡ç‚¹
MIN_SENTENCE_CHARS = int(os.environ.get("MIN_SENTENCE_CHARS", "4"))  # æœ€çŸ­å¥å­å­—ç¬¦æ•°
MAX_SENTENCE_SECONDS = float(os.environ.get("MAX_SENTENCE_SECONDS", "15"))  # æœ€é•¿å¥å­æ—¶é•¿
SEGMENT_GAP_THRESHOLD = float(os.environ.get("SEGMENT_GAP_THRESHOLD", "0.5"))  # segment é—´éš™é˜ˆå€¼


MODEL_ALIAS_MAP = {
    # Whisper GGML æ¨¡å‹åˆ«åæ˜ å°„
    "ggml-base.bin": "base",
    "ggml-small.bin": "small",
    "ggml-medium.bin": "medium",
    "ggml-large.bin": "large-v2",
    "ggml-large-v2.bin": "large-v2",
    "ggml-large-v3.bin": "large-v3",
}


def resolve_model_name() -> str:
    requested = os.environ.get("ASR_MODEL") or DEFAULT_MODEL_FALLBACK

    if not requested:
        return DEFAULT_MODEL_FALLBACK

    candidate = requested.strip()
    lower = candidate.lower()
    if lower in MODEL_ALIAS_MAP:
        return MODEL_ALIAS_MAP[lower]
    if "/" in candidate or lower.startswith("fast") or lower.startswith("systran/"):
        return candidate
    if lower in {"tiny", "base", "small", "medium", "large", "large-v2", "large-v3"}:
        return lower
    sys.stderr.write(f"[Worker] Unknown model alias '{candidate}', fallback to {DEFAULT_MODEL_FALLBACK}\n")
    sys.stderr.flush()
    return DEFAULT_MODEL_FALLBACK


DEVICE = os.environ.get("ASR_DEVICE", "cpu")
COMPUTE_TYPE = os.environ.get("ASR_COMPUTE_TYPE", "int8")
LANGUAGE = os.environ.get("ASR_LANGUAGE", "zh").strip() or None
BEAM_SIZE = int(os.environ.get("ASR_BEAM_SIZE", "5"))
TEMPERATURE = float(os.environ.get("ASR_TEMPERATURE", "0.0"))
VAD_FILTER = os.environ.get("ASR_VAD_FILTER", "1").lower() not in {"0", "false", "no"}
NO_SPEECH_THRESHOLD = float(os.environ.get("ASR_NO_SPEECH_THRESHOLD", "0.6"))
CACHE_DIR = os.environ.get("ASR_CACHE_DIR") or os.path.expanduser("~/.cache/faster-whisper")
os.makedirs(CACHE_DIR, exist_ok=True)


@dataclass
class SentenceBuffer:
    """å½“å‰æ­£åœ¨æ„å»ºçš„å¥å­"""
    text: str = ""
    start_time: float = 0.0  # å¥å­å¼€å§‹æ—¶é—´
    last_update_time: float = 0.0  # æœ€åæ›´æ–°æ—¶é—´


@dataclass
class SessionState:
    """
    ä¼šè¯çŠ¶æ€ï¼Œç®¡ç†éŸ³é¢‘ç¼“å†²å’Œåˆ†å¥
    
    ã€ä¼˜åŒ–ã€‘æ»‘åŠ¨çª—å£æ¨¡å¼ï¼š
    - ç´¯ç§¯éŸ³é¢‘ç›´åˆ°è¾¾åˆ°æœ€å°æ–°éŸ³é¢‘é˜ˆå€¼
    - åªè¯†åˆ«æœ€è¿‘ WINDOW_SECONDS çš„éŸ³é¢‘
    - é€šè¿‡æ–‡æœ¬å¯¹æ¯”æå–å¢é‡ç»“æœ
    """
    chunks: Deque[np.ndarray] = field(default_factory=deque)
    total_samples: int = 0
    
    # ã€ä¼˜åŒ–ã€‘æ»‘åŠ¨çª—å£çŠ¶æ€
    last_recognized_samples: int = 0  # ä¸Šæ¬¡è¯†åˆ«æ—¶çš„æ€»é‡‡æ ·æ•°
    last_recognized_text: str = ""    # ä¸Šæ¬¡è¯†åˆ«çš„å®Œæ•´æ–‡æœ¬
    pending_text: str = ""            # å¾…è¾“å‡ºçš„æ–‡æœ¬ï¼ˆç”¨äºåˆ†å¥ï¼‰
    
    # åˆ†å¥ç›¸å…³
    current_sentence: SentenceBuffer = field(default_factory=SentenceBuffer)
    sentence_start_time: float = 0.0  # å½“å‰å¥å­å¼€å§‹çš„æ—¶é—´æˆ³
    
    # å®Œæ•´å¥å­é˜Ÿåˆ—
    completed_sentences: List[str] = field(default_factory=list)

    def append_samples(self, samples: np.ndarray):
        """æ·»åŠ éŸ³é¢‘é‡‡æ ·ç‚¹"""
        self.chunks.append(samples)
        self.total_samples += len(samples)
        
        # é™åˆ¶æœ€å¤§ç¼“å†²
        while self.total_samples > MAX_BUFFER_SAMPLES and self.chunks:
            removed = self.chunks.popleft()
            self.total_samples -= len(removed)

    def get_new_audio_duration(self) -> float:
        """è·å–è‡ªä¸Šæ¬¡è¯†åˆ«ä»¥æ¥æ–°å¢çš„éŸ³é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰"""
        new_samples = self.total_samples - self.last_recognized_samples
        return new_samples / SAMPLE_RATE

    def should_recognize(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è¿›è¡Œè¯†åˆ«"""
        new_samples = self.total_samples - self.last_recognized_samples
        return new_samples >= MIN_NEW_AUDIO_SAMPLES

    def build_audio(self) -> Optional[np.ndarray]:
        """æ„å»ºå®Œæ•´éŸ³é¢‘æ•°ç»„"""
        if not self.chunks:
            return None
        if len(self.chunks) == 1:
            return self.chunks[0]
        return np.concatenate(list(self.chunks))

    def get_window_audio(self) -> Optional[np.ndarray]:
        """è·å–æ»‘åŠ¨çª—å£å†…çš„éŸ³é¢‘ï¼ˆæœ€è¿‘ WINDOW_SECONDSï¼‰"""
        audio = self.build_audio()
        if audio is None:
            return None
        
        if len(audio) <= WINDOW_SAMPLES:
            return audio
        
        # åªè¿”å›æœ€è¿‘çš„çª—å£
        return audio[-WINDOW_SAMPLES:]

    def mark_recognized(self, text: str):
        """æ ‡è®°å·²è¯†åˆ«ï¼Œæ›´æ–°çŠ¶æ€"""
        self.last_recognized_samples = self.total_samples
        self.last_recognized_text = text

    def clear_for_new_sentence(self):
        """æ¸…ç†çŠ¶æ€ï¼Œå‡†å¤‡æ¥æ”¶æ–°å¥å­"""
        # ä¿ç•™æœ€è¿‘ä¸€ç‚¹éŸ³é¢‘ä½œä¸ºä¸Šä¸‹æ–‡
        keep_samples = int(1.0 * SAMPLE_RATE)  # ä¿ç•™1ç§’
        audio = self.build_audio()
        
        if audio is not None and len(audio) > keep_samples:
            tail = audio[-keep_samples:]
            self.chunks = deque([tail])
            self.total_samples = keep_samples
        else:
            self.chunks.clear()
            self.total_samples = 0
        
        self.last_recognized_samples = self.total_samples
        self.last_recognized_text = ""
        self.pending_text = ""
        self.current_sentence = SentenceBuffer()
        self.sentence_start_time = time.time()

    def reset(self):
        """å®Œå…¨é‡ç½®çŠ¶æ€"""
        self.chunks.clear()
        self.total_samples = 0
        self.last_recognized_samples = 0
        self.last_recognized_text = ""
        self.pending_text = ""
        self.current_sentence = SentenceBuffer()
        self.sentence_start_time = time.time()
        self.completed_sentences.clear()


def decode_audio_chunk(audio_b64: str) -> np.ndarray:
    audio_bytes = base64.b64decode(audio_b64)
    audio_int16 = np.frombuffer(audio_bytes, dtype=np.int16)
    return audio_int16.astype(np.float32) / 32768.0


def extract_incremental_text(previous: str, current: str) -> str:
    """æå–å¢é‡æ–‡æœ¬ï¼ˆå½“å‰æ–‡æœ¬ç›¸å¯¹äºä¹‹å‰æ–‡æœ¬çš„æ–°å¢éƒ¨åˆ†ï¼‰"""
    if not current:
        return ""
    if not previous:
        return current
    if current == previous or current in previous:
        return ""
    if previous in current:
        return current[len(previous):]

    # å°è¯•æ‰¾åˆ°æœ€é•¿çš„é‡å éƒ¨åˆ†
    max_overlap = min(len(previous), len(current))
    for overlap in range(max_overlap, 0, -1):
        if previous[-overlap:] == current[:overlap]:
            return current[overlap:]
    return current


def find_sentence_boundaries(text: str) -> List[Tuple[int, str]]:
    """
    æ‰¾åˆ°æ–‡æœ¬ä¸­çš„å¥å­è¾¹ç•Œ
    è¿”å›: [(è¾¹ç•Œä½ç½®, è¾¹ç•Œç±»å‹), ...]
    è¾¹ç•Œç±»å‹: 'end' (å¥æœ«), 'clause' (åˆ†å¥)
    """
    boundaries = []
    for i, char in enumerate(text):
        if char in SENTENCE_END_PUNCTUATION:
            boundaries.append((i + 1, 'end'))
        elif char in CLAUSE_PUNCTUATION:
            boundaries.append((i + 1, 'clause'))
    return boundaries


def split_by_sentence_end(text: str) -> Tuple[List[str], str]:
    """
    æŒ‰å¥æœ«æ ‡ç‚¹åˆ†å‰²æ–‡æœ¬
    è¿”å›: (å®Œæ•´å¥å­åˆ—è¡¨, å‰©ä½™æ–‡æœ¬)
    """
    sentences = []
    remaining = text
    
    # ä½¿ç”¨æ­£åˆ™åŒ¹é…å¥æœ«æ ‡ç‚¹
    pattern = r'([^ã€‚ï¼ï¼Ÿ!?.ï¼›;]*[ã€‚ï¼ï¼Ÿ!?.ï¼›;])'
    matches = list(re.finditer(pattern, text))
    
    if not matches:
        return [], text
    
    last_end = 0
    for match in matches:
        sentence = match.group(1).strip()
        if sentence and len(sentence) >= MIN_SENTENCE_CHARS:
            sentences.append(sentence)
        last_end = match.end()
    
    remaining = text[last_end:].strip()
    return sentences, remaining


def load_model() -> WhisperModel:
    model_name = resolve_model_name()
    sys.stderr.write(f"[ASR Worker] Loading model: {model_name} (Faster-Whisper)\n")
    sys.stderr.write(f"[ASR Worker] Device={DEVICE}, compute_type={COMPUTE_TYPE}, cache={CACHE_DIR}\n")
    sys.stderr.flush()

    # é¦–å…ˆå°è¯•ä» HuggingFace åŠ è½½
    try:
        sys.stderr.write(f"[ASR Worker] Trying to load from HuggingFace: {model_name}\n")
        sys.stderr.flush()
        model = WhisperModel(
            model_name,
            device=DEVICE,
            compute_type=COMPUTE_TYPE,
            download_root=CACHE_DIR,
        )
        sys.stderr.write("[ASR Worker] Model loaded from HuggingFace\n")
        sys.stderr.flush()
        return model
    except Exception as hf_exc:
        sys.stderr.write(f"[ASR Worker] HuggingFace download failed: {hf_exc}\n")
        sys.stderr.write(f"[ASR Worker] Trying ModelScope mirror for: {model_name}\n")
        sys.stderr.flush()

        # å°è¯•ä» ModelScope é•œåƒæºåŠ è½½
        try:
            # æå–æ¨¡å‹å¤§å°ï¼ˆå¦‚æœmodel_nameåŒ…å«æ–œæ ï¼Œå–æœ€åä¸€éƒ¨åˆ†ï¼‰
            # ä¾‹å¦‚ "Xenova/whisper-base" -> "whisper-base"
            model_size = model_name.split('/')[-1] if '/' in model_name else model_name
            modelscope_repo = f"pengzhendong/faster-whisper-{model_size}"
            sys.stderr.write(f"[ASR Worker] Loading from ModelScope: {modelscope_repo}\n")
            sys.stderr.flush()
            model = WhisperModel(
                modelscope_repo,
                device=DEVICE,
                compute_type=COMPUTE_TYPE,
                download_root=CACHE_DIR,
            )
            sys.stderr.write("[ASR Worker] Model loaded from ModelScope\n")
            sys.stderr.flush()
            return model
        except Exception as ms_exc:
            sys.stderr.write(f"[ASR Worker] ModelScope download also failed: {ms_exc}\n")
            sys.stderr.write("[ASR Worker] Both HuggingFace and ModelScope failed, re-raising original error\n")
            sys.stderr.flush()
            # é‡æ–°æŠ›å‡ºåŸå§‹çš„ HuggingFace å¼‚å¸¸ï¼Œå› ä¸ºé‚£æ˜¯ç”¨æˆ·æœ€æœŸæœ›çš„æº
            raise hf_exc


def transcribe_audio_with_segments(model: WhisperModel, audio_source) -> Tuple[str, List[dict], dict]:
    """
    è½¬å½•éŸ³é¢‘å¹¶è¿”å› segment çº§åˆ«çš„ä¿¡æ¯
    è¿”å›: (å®Œæ•´æ–‡æœ¬, segmentsåˆ—è¡¨, info)
    """
    segments, info = model.transcribe(
        audio_source,
        beam_size=BEAM_SIZE,
        best_of=BEAM_SIZE,
        language=LANGUAGE,
        temperature=TEMPERATURE,
        vad_filter=VAD_FILTER,
        condition_on_previous_text=False,
        no_speech_threshold=NO_SPEECH_THRESHOLD,
        word_timestamps=False,  # å…³é—­ word-level timestamps ä»¥æé«˜é€Ÿåº¦
    )
    
    collected_segments = []
    collected_text = []
    
    for segment in segments:
        seg_text = segment.text.strip() if segment.text else ""
        if seg_text:
            collected_text.append(seg_text)
            collected_segments.append({
                "text": seg_text,
                "start": float(segment.start),
                "end": float(segment.end),
                "no_speech_prob": float(segment.no_speech_prob) if hasattr(segment, 'no_speech_prob') else 0.0,
            })
    
    full_text = "".join(collected_text).strip()
    return full_text, collected_segments, info


def handle_streaming_chunk(
    model: WhisperModel,
    data: Dict,
    sessions_cache: Dict[str, SessionState],
):
    """
    å¤„ç†æµå¼éŸ³é¢‘å—ï¼Œå®ç°æ··åˆåˆ†å¥ç­–ç•¥
    """
    request_id = data.get("request_id", "default")
    session_id = data.get("session_id", request_id)
    audio_data_b64 = data.get("audio_data")
    
    if not audio_data_b64:
        send_ipc_message({"request_id": request_id, "error": "No audio_data provided"})
        return

    state = sessions_cache.setdefault(session_id, SessionState())
    samples = decode_audio_chunk(audio_data_b64)
    state.append_samples(samples)

    is_final = bool(data.get("is_final", False))
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„éŸ³é¢‘è¿›è¡Œè¯†åˆ«
    if state.total_samples < MIN_DECODE_SAMPLES and not is_final:
        sys.stderr.write(f"[Worker] Not enough samples: {state.total_samples} < {MIN_DECODE_SAMPLES}, session={session_id}\n")
        sys.stderr.flush()
        return

    audio_array = state.build_audio()
    if audio_array is None or len(audio_array) == 0:
        sys.stderr.write(f"[Worker] No audio to process for session={session_id}\n")
        sys.stderr.flush()
        return

    timestamp_ms = int(time.time() * 1000)
    audio_duration = len(audio_array) / SAMPLE_RATE
    
    sys.stderr.write(f"[Worker] Starting transcription: session={session_id}, samples={len(audio_array)}, duration={audio_duration:.2f}s\n")
    sys.stderr.flush()

    try:
        full_text, segments, info = transcribe_audio_with_segments(model, audio_array)
        sys.stderr.write(f"[Worker] Transcription result: text=\"{full_text[:50] if full_text else '(empty)'}...\", segments={len(segments)}\n")
        sys.stderr.flush()
    except Exception as exc:
        sys.stderr.write(f"[Worker] Streaming decode failed: {exc}\n")
        sys.stderr.write(traceback.format_exc())
        sys.stderr.flush()
        send_ipc_message({
            "request_id": request_id,
            "session_id": session_id,
            "error": str(exc),
        })
        return

    if not full_text:
        sys.stderr.write(f"[Worker] Empty transcription result for session={session_id}\n")
        sys.stderr.flush()
        return

    # ==============================================================================
    # æ··åˆåˆ†å¥ç­–ç•¥
    # ==============================================================================
    
    # 1. æ£€æµ‹ Whisper segment è¾¹ç•Œï¼ˆVAD å±‚é¢çš„æ–­å¥ï¼‰
    segment_boundaries = []
    if len(segments) > 1:
        for i in range(1, len(segments)):
            gap = segments[i]["start"] - segments[i-1]["end"]
            if gap >= SEGMENT_GAP_THRESHOLD:
                # æ‰¾åˆ°ä¸€ä¸ªæ˜¾è‘—çš„åœé¡¿
                segment_boundaries.append({
                    "position": sum(len(s["text"]) for s in segments[:i]),
                    "gap": gap,
                    "time": segments[i-1]["end"],
                })
    
    # 2. æ£€æµ‹å¥æœ«æ ‡ç‚¹
    complete_sentences, remaining_text = split_by_sentence_end(full_text)
    
    # 3. æ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å¤§å¥å­æ—¶é•¿
    sentence_duration = (state.total_samples - state.sentence_start_sample) / SAMPLE_RATE
    force_commit = sentence_duration >= MAX_SENTENCE_SECONDS
    
    # 4. å†³å®šæ˜¯å¦æäº¤å¥å­
    sentences_to_commit = []
    
    # 4.1 å¦‚æœæœ‰å®Œæ•´å¥å­ï¼ˆä»¥å¥æœ«æ ‡ç‚¹ç»“å°¾ï¼‰ï¼Œæäº¤å®ƒä»¬
    if complete_sentences:
        sentences_to_commit.extend(complete_sentences)
        # æ›´æ–°å½“å‰å¥å­ä¸ºå‰©ä½™æ–‡æœ¬
        state.current_sentence.text = remaining_text
        state.committed_text_length = len(full_text) - len(remaining_text)
    
    # 4.2 å¦‚æœæœ‰æ˜¾è‘—çš„ segment è¾¹ç•Œä¸”å½“å‰å¥å­è¶³å¤Ÿé•¿ï¼Œä¹Ÿå¯ä»¥æ–­å¥
    elif segment_boundaries and len(state.current_sentence.text) >= MIN_SENTENCE_CHARS * 2:
        # ä½¿ç”¨æœ€åä¸€ä¸ª segment è¾¹ç•Œè¿›è¡Œæ–­å¥
        last_boundary = segment_boundaries[-1]
        boundary_pos = last_boundary["position"]
        if boundary_pos > MIN_SENTENCE_CHARS:
            sentence_part = full_text[:boundary_pos].strip()
            if sentence_part and len(sentence_part) >= MIN_SENTENCE_CHARS:
                sentences_to_commit.append(sentence_part)
                state.current_sentence.text = full_text[boundary_pos:].strip()
                state.committed_text_length = boundary_pos
    
    # 4.3 å¦‚æœè¶…è¿‡æœ€å¤§æ—¶é•¿ï¼Œå¼ºåˆ¶æäº¤å½“å‰æ–‡æœ¬
    elif force_commit and full_text and len(full_text) >= MIN_SENTENCE_CHARS:
        sentences_to_commit.append(full_text)
        state.current_sentence.text = ""
        state.committed_text_length = len(full_text)
    
    # 4.4 å¦‚æœæ˜¯æœ€ç»ˆå—ï¼Œæäº¤æ‰€æœ‰å‰©ä½™æ–‡æœ¬
    elif is_final and full_text and len(full_text) >= MIN_SENTENCE_CHARS:
        sentences_to_commit.append(full_text)
        state.current_sentence.text = ""
        state.committed_text_length = len(full_text)
    
    # 5. å‘é€å®Œæ•´å¥å­äº‹ä»¶
    for sentence in sentences_to_commit:
        if sentence and len(sentence.strip()) >= MIN_SENTENCE_CHARS:
            sys.stderr.write(f"[Worker] ğŸ¯ SENTENCE_COMPLETE: \"{sentence.strip()[:50]}...\" (session={session_id})\n")
            sys.stderr.flush()
            send_ipc_message({
                "request_id": request_id,
                "session_id": session_id,
                "type": "sentence_complete",
                "text": sentence.strip(),
                "timestamp": timestamp_ms,
                "is_final": is_final,
                "status": "success",
                "language": info.language if hasattr(info, "language") else None,
                "audio_duration": audio_duration,
            })
            # æ¸…ç†å·²æäº¤å¥å­å¯¹åº”çš„éŸ³é¢‘
            state.clear_audio_before(LOOKBACK_SECONDS)
            state.sentence_start_sample = 0
    
    # 6. å‘é€å®æ—¶å­—å¹•ï¼ˆå¢é‡æ–‡æœ¬ï¼‰
    current_text = state.current_sentence.text if state.current_sentence.text else (
        remaining_text if complete_sentences else full_text
    )
    
    # è®¡ç®—å¢é‡æ–‡æœ¬
    if not sentences_to_commit:
        # æ²¡æœ‰æäº¤å¥å­ï¼Œå‘é€å¢é‡å­—å¹•
        incremental = extract_incremental_text(
            state.current_sentence.text if state.current_sentence.text else "",
            current_text
        ).strip()
        
        if incremental:
            sys.stderr.write(f"[Worker] ğŸ“ PARTIAL: \"{incremental[:30]}...\" full=\"{current_text[:30]}...\" (session={session_id})\n")
            sys.stderr.flush()
            send_ipc_message({
                "request_id": request_id,
                "session_id": session_id,
                "type": "partial",
                "text": incremental,
                "full_text": current_text,
                "timestamp": timestamp_ms,
                "is_final": is_final,
                "status": "success",
                "language": info.language if hasattr(info, "language") else None,
            })
    
    # æ›´æ–°çŠ¶æ€
    state.current_sentence.text = current_text
    state.current_sentence.last_update_time = time.time()


def handle_batch_file(model: WhisperModel, data: Dict):
    """å¤„ç†æ‰¹é‡æ–‡ä»¶è¯†åˆ«"""
    request_id = data.get("request_id", "unknown")
    audio_path = data.get("audio_path")

    if not audio_path:
        send_ipc_message({"request_id": request_id, "error": "No audio_path provided"})
        return
    if not os.path.exists(audio_path):
        send_ipc_message({"request_id": request_id, "error": f"File not found: {audio_path}"})
        return

    try:
        text, segments, info = transcribe_audio_with_segments(model, audio_path)
    except Exception as exc:
        send_ipc_message({
            "request_id": request_id,
            "error": str(exc),
            "traceback": traceback.format_exc(),
        })
        return

    send_ipc_message({
        "request_id": request_id,
        "text": text,
        "segments": segments,
        "language": info.language if hasattr(info, "language") else None,
        "status": "success",
    })


def handle_force_commit(data: Dict, sessions_cache: Dict[str, SessionState]):
    """
    å¼ºåˆ¶æäº¤å½“å‰å¥å­ï¼ˆç”± JS ä¾§é™éŸ³æ£€æµ‹è§¦å‘ï¼‰
    """
    request_id = data.get("request_id", "default")
    session_id = data.get("session_id", request_id)
    
    sys.stderr.write(f"[Worker] force_commit received for session={session_id}\n")
    sys.stderr.flush()
    
    state = sessions_cache.get(session_id)
    if not state:
        sys.stderr.write(f"[Worker] No session state found for session={session_id}\n")
        sys.stderr.flush()
        return
    
    current_text = state.current_sentence.text.strip()
    sys.stderr.write(f"[Worker] force_commit current_sentence=\"{current_text[:50] if current_text else '(empty)'}...\" len={len(current_text)}\n")
    sys.stderr.flush()
    
    if current_text and len(current_text) >= MIN_SENTENCE_CHARS:
        timestamp_ms = int(time.time() * 1000)
        sys.stderr.write(f"[Worker] ğŸ¯ FORCE_COMMIT SENTENCE: \"{current_text[:50]}...\" (session={session_id})\n")
        sys.stderr.flush()
        send_ipc_message({
            "request_id": request_id,
            "session_id": session_id,
            "type": "sentence_complete",
            "text": current_text,
            "timestamp": timestamp_ms,
            "is_final": True,
            "status": "success",
            "trigger": "silence_timeout",
        })
        
        # é‡ç½®çŠ¶æ€
        state.current_sentence = SentenceBuffer()
        state.committed_text_length = 0
        state.clear_audio_before(LOOKBACK_SECONDS)
        state.sentence_start_sample = 0
    else:
        sys.stderr.write(f"[Worker] force_commit: text too short or empty, not sending (min={MIN_SENTENCE_CHARS})\n")
        sys.stderr.flush()


def main():
    try:
        model = load_model()
        sessions_cache: Dict[str, SessionState] = {}
        send_ipc_message({"status": "ready"})

        while True:
            line = sys.stdin.readline()
            if not line:
                break
            try:
                data = json.loads(line)
            except json.JSONDecodeError as exc:
                send_ipc_message({"request_id": "unknown", "error": f"Invalid JSON: {exc}"})
                continue

            request_type = data.get("type")
            request_id = data.get("request_id", "default")
            session_id = data.get("session_id", request_id)
            
            # æ¯éš”ä¸€å®šæ•°é‡æ‰“å°æ”¶åˆ°çš„è¯·æ±‚ç±»å‹
            if request_type == "streaming_chunk":
                # æµå¼chunkä¸æ¯æ¬¡éƒ½æ‰“å°ï¼Œé¿å…åˆ·å±
                pass
            else:
                sys.stderr.write(f"[Worker] Received request: type={request_type}, session={session_id}\n")
                sys.stderr.flush()

            if request_type == "reset_session":
                sys.stderr.write(f"[Worker] Resetting session: {session_id}\n")
                sys.stderr.flush()
                sessions_cache.pop(session_id, None)
                continue

            if request_type == "force_commit":
                # JS ä¾§é™éŸ³æ£€æµ‹è§¦å‘çš„å¼ºåˆ¶æäº¤
                handle_force_commit(data, sessions_cache)
                continue

            if request_type == "streaming_chunk":
                handle_streaming_chunk(model, data, sessions_cache)
                continue

            if request_type == "batch_file" or "audio_path" in data:
                handle_batch_file(model, data)
                continue

            send_ipc_message({
                "request_id": request_id,
                "error": f"Unknown request type: {request_type}",
            })
    except Exception as exc:
        sys.stderr.write(f"[Worker] Fatal error: {exc}\n")
        sys.stderr.write(traceback.format_exc())
        sys.stderr.flush()
        send_ipc_message({"status": "fatal", "error": str(exc)})
        sys.exit(1)


if __name__ == "__main__":
    main()
